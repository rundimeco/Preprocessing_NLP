The objective of this course is to evaluate the influence of ``preprocessing`` in NLP by first focusing on text classification and a classical Machine Learning Pipeline.

This repository contains :

- Slides in PDF : `Pre_processing.pdf`
- A simple notebook to quickly compare different pre-processing techniques:
  - `01_run_experiments_simple_single_task.ipynb` (Kaggle dataset on text categorization)
- Another example with a multilingual task (language identification using `corpus_muli.zip`):
  - 01_DiagLang.ipynb
- Now it's up to you to find the best pre-processing configuration for two other tasks :
  - https://www.kaggle.com/datasets/suraj520/multi-task-learning
  - https://www.kaggle.com/competitions/author-attribution-challenge-insignia-2024
- The objective is to try different classification models (ML, language models) with differ preprocessing configurations in order to find :
  - What preprocessing combinations work best
  - How much this depends on the learning method 
