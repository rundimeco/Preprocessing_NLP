{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b262b75c",
   "metadata": {},
   "source": [
    "# NLP Lab â€“ Minimal Preprocessing Experiments (Single Task)\n",
    "\n",
    "This notebook runs a tiny grid of preprocessing experiments on **one** supervised text classification task\n",
    "and saves results to `SEED = 42\n",
    "OUTFILE = Path(\"results_autorship-attribution.csv\")`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96259f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy \n",
    "import pandas\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aad0a76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45b8320f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stopwordsiso in /home/ceres/anaconda3/lib/python3.10/site-packages (0.6.1)\n",
      "Prep: DON\n",
      "I am travelling to Nancy for an NLP course at IDMC :https://idmc.univ-lorraine.fr/\n",
      "Prep: LOW\n",
      "i am travelling to nancy for an nlp course at idmc :https://idmc.univ-lorraine.fr/\n",
      "Prep: URLrem\n",
      "I am travelling to Nancy for an NLP course at IDMC : \n",
      "Prep: URLrep\n",
      "I am travelling to Nancy for an NLP course at IDMC : <URL> \n",
      "Prep: PUN\n",
      "I am travelling to Nancy for an NLP course at IDMC  https   idmc univ lorraine fr \n",
      "Prep: RSW\n",
      "I travelling Nancy NLP IDMC : https : / / idmc . univ - lorraine . /\n",
      "Prep: LOW+URLrem\n",
      "i am travelling to nancy for an nlp course at idmc :\n",
      "Prep: LOW+URLrep\n",
      "i am travelling to nancy for an nlp course at idmc : <URL>\n",
      "Prep: LOW+PUN\n",
      "i am travelling to nancy for an nlp course at idmc https idmc univ lorraine fr\n",
      "Prep: LOW+URLrem+PUN\n",
      "i am travelling to nancy for an nlp course at idmc\n",
      "Prep: LOW+URLrep+PUN\n",
      "i am travelling to nancy for an nlp course at idmc\n",
      "Prep: LOW+URLrem+PUN+RSW\n",
      "travelling nancy nlp idmc\n",
      "Prep: LOW+URLrep+PUN+RSW\n",
      "travelling nancy nlp idmc URL\n"
     ]
    }
   ],
   "source": [
    "# --- Preprocessing (kept simple) ---\n",
    "\n",
    "!pip install stopwordsiso\n",
    "import stopwordsiso\n",
    "\n",
    "def don(text): \n",
    "    return text\n",
    "    \n",
    "def lower(text): \n",
    "    return text.lower()\n",
    "\n",
    "URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "def remove_urls(text): \n",
    "    return URL_RE.sub(\" \", text)\n",
    "    \n",
    "def replace_urls(text): \n",
    "    return URL_RE.sub(\" <URL> \", text)\n",
    "    \n",
    "PUNCT_RE = re.compile(r\"[^\\w\\s]\")\n",
    "def remove_punct(text): \n",
    "    return PUNCT_RE.sub(\" \", text)\n",
    "\n",
    "TOKEN_PUNC = re.compile(r\"\\w+|[^\\w\\s]\")\n",
    "def remove_stopwords(text):\n",
    "    #very slow but keeps punctuation\n",
    "    \n",
    "    return \" \".join([T for T in TOKEN_PUNC.findall(text) if T not in set(stopwordsiso.stopwords(\"en\"))])\n",
    "\n",
    "    \n",
    "def compose(*funcs):\n",
    "    def f(text):\n",
    "        for fn in funcs:\n",
    "            text = fn(text)\n",
    "        return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return f\n",
    "\n",
    "PREPROCESSORS = {\n",
    "    \"DON\": don,\n",
    "    \"LOW\": lower,\n",
    "    \"URLrem\": remove_urls,\n",
    "    \"URLrep\": replace_urls,\n",
    "    \"PUN\": remove_punct,\n",
    "    \"RSW\": remove_stopwords,\n",
    "    \"LOW+URLrem\": compose(lower, remove_urls),\n",
    "    \"LOW+URLrep\": compose(lower, replace_urls),\n",
    "    \"LOW+PUN\": compose(lower, remove_punct),\n",
    "    \"LOW+URLrem+PUN\": compose(lower, remove_urls, remove_punct),\n",
    "    \"LOW+URLrep+PUN\": compose(lower, remove_urls, remove_punct),\n",
    "    \"LOW+URLrem+PUN+RSW\": compose(lower, remove_urls, remove_punct, remove_stopwords),\n",
    "    \"LOW+URLrep+PUN+RSW\": compose(lower, replace_urls, remove_punct, remove_stopwords),\n",
    "\n",
    "}\n",
    "toto = \"I am travelling to Nancy for an NLP course at IDMC :https://idmc.univ-lorraine.fr/\" \n",
    "for prep_name, prep in PREPROCESSORS.items():\n",
    "    print(f\"Prep: {prep_name}\")\n",
    "    print(prep(toto))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f18f0c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.4.1).\n",
      "Local Path to dataset files: /home/ceres/.cache/kagglehub/datasets/azimulh/tweets-data-for-authorship-attribution-modelling/versions/2\n",
      "Samples: 9908 Classes: {'Sebastian Ruder', 'KATY PERRY', 'Ellen DeGeneres', 'Barack Obama', 'Neil deGrasse Tyson'}\n"
     ]
    }
   ],
   "source": [
    "#!pip install kagglehub\n",
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"azimulh/tweets-data-for-authorship-attribution-modelling\")\n",
    "\n",
    "print(\"Local Path to dataset files:\", path)\n",
    "import glob\n",
    "csv_data = pandas.read_csv(f\"{path}/tweet_with_authors.csv\")\n",
    "\n",
    "X = csv_data[\"tweet\"]\n",
    "y = csv_data[\"author\"]\n",
    "\n",
    "print(\"Samples:\", len(X), \"Classes:\", set(y) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35a0118d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation part\n",
    "macro_f1 = make_scorer(f1_score, average=\"macro\")\n",
    "SCORING = {\"acc\": \"accuracy\", \"macro_f1\": macro_f1}\n",
    "\n",
    "def mean_scores(scores):\n",
    "    return {k.replace(\"test_\", \"\"): float(numpy.mean(v))\n",
    "            for k, v in scores.items() if k.startswith(\"test_\")}\n",
    "\n",
    "def evaluate(X, y, preprocess, vectorizer):\n",
    "    Xp = [preprocess(t) for t in X]\n",
    "    pipe = Pipeline([\n",
    "        (\"vect\", vectorizer),\n",
    "        (\"clf\", MODEL),\n",
    "    ])\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    scores = cross_validate(pipe, Xp, y, cv=cv, scoring=SCORING, n_jobs=-1)\n",
    "    return mean_scores(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4af70d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prep: DON          | Vec: count_word_1-1\n",
      "{'acc': 0.8767650398258734, 'macro_f1': 0.8766916963023161}\n",
      "Prep: DON          | Vec: tfidf_word_1-1\n",
      "{'acc': 0.8706095393625924, 'macro_f1': 0.8688717054716129}\n",
      "Prep: DON          | Vec: count_char_3-5\n",
      "{'acc': 0.9173392944374179, 'macro_f1': 0.9171911709273759}\n",
      "Prep: DON          | Vec: tfidf_char_3-5\n",
      "{'acc': 0.9065400314083695, 'macro_f1': 0.9057020681441363}\n",
      "Prep: LOW          | Vec: count_word_1-1\n",
      "{'acc': 0.874242997680793, 'macro_f1': 0.8741392877735237}\n",
      "Prep: LOW          | Vec: tfidf_word_1-1\n",
      "{'acc': 0.870710345660159, 'macro_f1': 0.8688157437746675}\n",
      "Prep: LOW          | Vec: count_char_3-5\n",
      "{'acc': 0.9152198152886324, 'macro_f1': 0.9150822527896427}\n",
      "Prep: LOW          | Vec: tfidf_char_3-5\n",
      "{'acc': 0.9053289805116315, 'macro_f1': 0.9044079103264464}\n",
      "Prep: URLrem       | Vec: count_word_1-1\n",
      "{'acc': 0.8661674403299561, 'macro_f1': 0.8663700245342051}\n",
      "Prep: URLrem       | Vec: tfidf_word_1-1\n",
      "{'acc': 0.8600112776727041, 'macro_f1': 0.8590353442354731}\n",
      "Prep: URLrem       | Vec: count_char_3-5\n",
      "{'acc': 0.9040178364492955, 'macro_f1': 0.90381242265398}\n",
      "Prep: URLrem       | Vec: tfidf_char_3-5\n",
      "{'acc': 0.9001815939620135, 'macro_f1': 0.8997252437639875}\n",
      "Prep: URLrep       | Vec: count_word_1-1\n",
      "{'acc': 0.870608367788644, 'macro_f1': 0.8706838607606275}\n",
      "Prep: URLrep       | Vec: tfidf_word_1-1\n",
      "{'acc': 0.8682872760447257, 'macro_f1': 0.8668840965798653}\n",
      "Prep: URLrep       | Vec: count_char_3-5\n",
      "{'acc': 0.9167344566520186, 'macro_f1': 0.9165829438188196}\n",
      "Prep: URLrep       | Vec: tfidf_char_3-5\n",
      "{'acc': 0.9125959990240279, 'macro_f1': 0.912147433055113}\n",
      "Prep: PUN          | Vec: count_word_1-1\n",
      "{'acc': 0.8767650398258734, 'macro_f1': 0.8766916963023161}\n",
      "Prep: PUN          | Vec: tfidf_word_1-1\n",
      "{'acc': 0.8706095393625924, 'macro_f1': 0.8688717054716129}\n",
      "Prep: PUN          | Vec: count_char_3-5\n",
      "{'acc': 0.8951351665239555, 'macro_f1': 0.8950284870832259}\n",
      "Prep: PUN          | Vec: tfidf_char_3-5\n",
      "{'acc': 0.8861517921770442, 'macro_f1': 0.8850444622104636}\n",
      "Prep: RSW          | Vec: count_word_1-1\n",
      "{'acc': 0.8658661420732072, 'macro_f1': 0.8664455048208068}\n",
      "Prep: RSW          | Vec: tfidf_word_1-1\n",
      "{'acc': 0.8597090625319954, 'macro_f1': 0.8584411232045841}\n",
      "Prep: RSW          | Vec: count_char_3-5\n",
      "{'acc': 0.9213772513958285, 'macro_f1': 0.9213674300802944}\n",
      "Prep: RSW          | Vec: tfidf_char_3-5\n",
      "{'acc': 0.9070452854081484, 'macro_f1': 0.9068645135137968}\n",
      "Prep: LOW+URLrem   | Vec: count_word_1-1\n",
      "{'acc': 0.8642513566062254, 'macro_f1': 0.864323882451093}\n",
      "Prep: LOW+URLrem   | Vec: tfidf_word_1-1\n",
      "{'acc': 0.8626360108212683, 'macro_f1': 0.8614022968474474}\n",
      "Prep: LOW+URLrem   | Vec: count_char_3-5\n",
      "{'acc': 0.8965492562797637, 'macro_f1': 0.896323774523756}\n",
      "Prep: LOW+URLrem   | Vec: tfidf_char_3-5\n",
      "{'acc': 0.896951615524068, 'macro_f1': 0.8964795358785936}\n",
      "Prep: LOW+URLrep   | Vec: count_word_1-1\n",
      "{'acc': 0.8708115085237098, 'macro_f1': 0.8706474400994502}\n",
      "Prep: LOW+URLrep   | Vec: tfidf_word_1-1\n",
      "{'acc': 0.8703068148419064, 'macro_f1': 0.8685733868187363}\n",
      "Prep: LOW+URLrep   | Vec: count_char_3-5\n",
      "{'acc': 0.9139085693503011, 'macro_f1': 0.9137161708708625}\n",
      "Prep: LOW+URLrep   | Vec: tfidf_char_3-5\n",
      "{'acc': 0.9119901424786736, 'macro_f1': 0.9114322502365935}\n",
      "Prep: LOW+PUN      | Vec: count_word_1-1\n",
      "{'acc': 0.874242997680793, 'macro_f1': 0.8741392877735237}\n",
      "Prep: LOW+PUN      | Vec: tfidf_word_1-1\n",
      "{'acc': 0.870710345660159, 'macro_f1': 0.8688157437746675}\n",
      "Prep: LOW+PUN      | Vec: count_char_3-5\n",
      "{'acc': 0.8835290456103925, 'macro_f1': 0.8833832819769967}\n",
      "Prep: LOW+PUN      | Vec: tfidf_char_3-5\n",
      "{'acc': 0.8811058231809659, 'macro_f1': 0.8799153718188626}\n",
      "Prep: LOW+URLrem+PUN | Vec: count_word_1-1\n",
      "{'acc': 0.8642513566062254, 'macro_f1': 0.864323882451093}\n",
      "Prep: LOW+URLrem+PUN | Vec: tfidf_word_1-1\n",
      "{'acc': 0.8626360108212683, 'macro_f1': 0.8614022968474474}\n",
      "Prep: LOW+URLrem+PUN | Vec: count_char_3-5\n",
      "{'acc': 0.8736378033294094, 'macro_f1': 0.8735794996438544}\n",
      "Prep: LOW+URLrem+PUN | Vec: tfidf_char_3-5\n",
      "{'acc': 0.8769679768089483, 'macro_f1': 0.8764500740363305}\n",
      "Prep: LOW+URLrep+PUN | Vec: count_word_1-1\n",
      "{'acc': 0.8642513566062254, 'macro_f1': 0.864323882451093}\n",
      "Prep: LOW+URLrep+PUN | Vec: tfidf_word_1-1\n",
      "{'acc': 0.8626360108212683, 'macro_f1': 0.8614022968474474}\n",
      "Prep: LOW+URLrep+PUN | Vec: count_char_3-5\n",
      "{'acc': 0.8736378033294094, 'macro_f1': 0.8735794996438544}\n",
      "Prep: LOW+URLrep+PUN | Vec: tfidf_char_3-5\n",
      "{'acc': 0.8769679768089483, 'macro_f1': 0.8764500740363305}\n",
      "Prep: LOW+URLrem+PUN+RSW | Vec: count_word_1-1\n",
      "{'acc': 0.8557727778171133, 'macro_f1': 0.8567156193124941}\n",
      "Prep: LOW+URLrem+PUN+RSW | Vec: tfidf_word_1-1\n",
      "{'acc': 0.8601122877222617, 'macro_f1': 0.8591368386567044}\n",
      "Prep: LOW+URLrem+PUN+RSW | Vec: count_char_3-5\n",
      "{'acc': 0.8544611753127975, 'macro_f1': 0.8547585614278319}\n",
      "Prep: LOW+URLrem+PUN+RSW | Vec: tfidf_char_3-5\n",
      "{'acc': 0.8641502446806723, 'macro_f1': 0.8639274100549438}\n",
      "Prep: LOW+URLrep+PUN+RSW | Vec: count_word_1-1\n",
      "{'acc': 0.8612229398254151, 'macro_f1': 0.8615581454309857}\n",
      "Prep: LOW+URLrep+PUN+RSW | Vec: tfidf_word_1-1\n",
      "{'acc': 0.8621309605734805, 'macro_f1': 0.860754245994303}\n",
      "Prep: LOW+URLrep+PUN+RSW | Vec: count_char_3-5\n",
      "{'acc': 0.8692982934242611, 'macro_f1': 0.8692395061499276}\n",
      "Prep: LOW+URLrep+PUN+RSW | Vec: tfidf_char_3-5\n",
      "{'acc': 0.8731323455776396, 'macro_f1': 0.8724941288976995}\n"
     ]
    }
   ],
   "source": [
    "#Partie apprentissage\n",
    "MODEL = LogisticRegression(max_iter=2000, random_state=SEED)\n",
    "SEED = 42\n",
    "OUTFILE = Path(\"results_autorship-attribution.csv\")\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore:pkg_resources is deprecated as an API:UserWarning\"\n",
    "\n",
    "\n",
    "VECTORIZERS = {\n",
    "    \"count_word_1-1\": CountVectorizer(analyzer=\"word\", ngram_range=(1, 1), lowercase=False),\n",
    "    #\"count_word_1-2\": CountVectorizer(analyzer=\"word\", ngram_range=(1, 2), lowercase=False),\n",
    "    \"tfidf_word_1-1\": TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 1), lowercase=False),\n",
    "    #\"tfidf_word_1-2\": TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 2), lowercase=False),\n",
    "    \"count_char_3-5\": CountVectorizer(analyzer=\"char\", ngram_range=(3, 5), lowercase=False),\n",
    "    #\"count_charwb_3-5\": CountVectorizer(analyzer=\"char_wb\", ngram_range=(3, 5), lowercase=False),\n",
    "    \"tfidf_char_3-5\": TfidfVectorizer(analyzer=\"char\", ngram_range=(3, 5), lowercase=False),\n",
    "    #\"tfidf_charwb_3-5\": TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3, 5), lowercase=False),\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for prep_name, prep in PREPROCESSORS.items():\n",
    "    for vec_name, vec in VECTORIZERS.items():\n",
    "        print(f\"Prep: {prep_name:12s} | Vec: {vec_name}\")\n",
    "        res = evaluate(X, y, prep, vec)\n",
    "        print(res)\n",
    "        rows.append({\n",
    "            \"preprocessing\": prep_name,\n",
    "            \"vectorizer\": vec_name,\n",
    "            **res\n",
    "        })\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe8d0203-210d-4471-928c-4064db7d2d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         preprocessing      vectorizer       acc  macro_f1\n",
      "49  LOW+URLrep+PUN+RSW  tfidf_word_1-1  0.912542  0.912576\n",
      "45  LOW+URLrem+PUN+RSW  tfidf_word_1-1  0.912542  0.912576\n",
      "21                 RSW  tfidf_word_1-1  0.904746  0.905020\n",
      "47  LOW+URLrem+PUN+RSW  tfidf_char_3-5  0.901017  0.900910\n",
      "51  LOW+URLrep+PUN+RSW  tfidf_char_3-5  0.898644  0.898668\n",
      "5                  LOW  tfidf_word_1-1  0.892542  0.892805\n",
      "33             LOW+PUN  tfidf_word_1-1  0.892542  0.892805\n",
      "25          LOW+URLrem  tfidf_word_1-1  0.892542  0.892801\n",
      "37      LOW+URLrem+PUN  tfidf_word_1-1  0.892542  0.892801\n",
      "41      LOW+URLrep+PUN  tfidf_word_1-1  0.892542  0.892801\n",
      "29          LOW+URLrep  tfidf_word_1-1  0.892203  0.892461\n",
      "39      LOW+URLrem+PUN  tfidf_char_3-5  0.891864  0.892094\n",
      "43      LOW+URLrep+PUN  tfidf_char_3-5  0.891864  0.892094\n",
      "19                 PUN  tfidf_char_3-5  0.891864  0.892075\n",
      "35             LOW+PUN  tfidf_char_3-5  0.891186  0.891393\n",
      "48  LOW+URLrep+PUN+RSW  count_word_1-1  0.890169  0.890619\n",
      "44  LOW+URLrem+PUN+RSW  count_word_1-1  0.889831  0.890276\n",
      "1                  DON  tfidf_word_1-1  0.888475  0.888799\n",
      "13              URLrep  tfidf_word_1-1  0.888475  0.888799\n",
      "17                 PUN  tfidf_word_1-1  0.888475  0.888799\n",
      "9               URLrem  tfidf_word_1-1  0.887458  0.887775\n",
      "23                 RSW  tfidf_char_3-5  0.882034  0.882015\n",
      "31          LOW+URLrep  tfidf_char_3-5  0.877627  0.877797\n",
      "27          LOW+URLrem  tfidf_char_3-5  0.877627  0.877775\n",
      "20                 RSW  count_word_1-1  0.875254  0.875613\n",
      "7                  LOW  tfidf_char_3-5  0.874576  0.874761\n",
      "15              URLrep  tfidf_char_3-5  0.872203  0.872389\n",
      "3                  DON  tfidf_char_3-5  0.872203  0.872388\n",
      "11              URLrem  tfidf_char_3-5  0.871864  0.872051\n",
      "50  LOW+URLrep+PUN+RSW  count_char_3-5  0.865763  0.866171\n",
      "46  LOW+URLrem+PUN+RSW  count_char_3-5  0.864746  0.865134\n",
      "24          LOW+URLrem  count_word_1-1  0.859322  0.859535\n",
      "40      LOW+URLrep+PUN  count_word_1-1  0.859322  0.859535\n",
      "28          LOW+URLrep  count_word_1-1  0.859322  0.859535\n",
      "36      LOW+URLrem+PUN  count_word_1-1  0.859322  0.859535\n",
      "32             LOW+PUN  count_word_1-1  0.858983  0.859198\n",
      "4                  LOW  count_word_1-1  0.858983  0.859198\n",
      "22                 RSW  count_char_3-5  0.857627  0.857866\n",
      "18                 PUN  count_char_3-5  0.856271  0.856460\n",
      "38      LOW+URLrem+PUN  count_char_3-5  0.854237  0.854614\n",
      "42      LOW+URLrep+PUN  count_char_3-5  0.854237  0.854614\n",
      "34             LOW+PUN  count_char_3-5  0.853559  0.853893\n",
      "0                  DON  count_word_1-1  0.852203  0.852437\n",
      "16                 PUN  count_word_1-1  0.852203  0.852437\n",
      "10              URLrem  count_char_3-5  0.851525  0.851809\n",
      "12              URLrep  count_word_1-1  0.851525  0.851773\n",
      "8               URLrem  count_word_1-1  0.851186  0.851428\n",
      "2                  DON  count_char_3-5  0.850847  0.851119\n",
      "14              URLrep  count_char_3-5  0.850169  0.850447\n",
      "30          LOW+URLrep  count_char_3-5  0.848136  0.848456\n",
      "6                  LOW  count_char_3-5  0.847797  0.848063\n",
      "26          LOW+URLrem  count_char_3-5  0.846780  0.847134\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(rows).sort_values(\"macro_f1\", ascending=False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb4790cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: results_single_task_3_classes.csv\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(OUTFILE, index=False)\n",
    "print(f\"Saved to: {OUTFILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24da9072-5156-40fa-9fda-c4c3048f8391",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = LogisticRegression(max_iter=200, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16ac99a6-74fe-44d7-b7b3-2cf8ae2636cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: 18846 Classes: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19} Labels: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "Prep: LOW          | Vec: tfidf_word_1-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ceres/anaconda3/lib/python3.10/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  return _ForkingPickler.loads(res)\n",
      "/home/ceres/anaconda3/lib/python3.10/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  return _ForkingPickler.loads(res)\n",
      "/home/ceres/anaconda3/lib/python3.10/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc': 0.7311891016550626, 'macro_f1': 0.7195677702933339}\n",
      "Prep: URLrem       | Vec: tfidf_word_1-1\n",
      "{'acc': 0.7207887886168962, 'macro_f1': 0.7101215641322157}\n",
      "Prep: URLrep       | Vec: tfidf_word_1-1\n",
      "{'acc': 0.7208949175635665, 'macro_f1': 0.7102590301600126}\n",
      "Prep: PUN          | Vec: tfidf_word_1-1\n",
      "{'acc': 0.7203642728302155, 'macro_f1': 0.7095112650794365}\n",
      "Prep: RSW          | Vec: tfidf_word_1-1\n",
      "{'acc': 0.7391483363161573, 'macro_f1': 0.7282771443235193}\n",
      "Prep: LOW+URLrem   | Vec: tfidf_word_1-1\n",
      "{'acc': 0.7310299363859716, 'macro_f1': 0.7196131318136788}\n",
      "Prep: LOW+URLrep   | Vec: tfidf_word_1-1\n",
      "{'acc': 0.7303400560062439, 'macro_f1': 0.7187658683953233}\n",
      "Prep: LOW+PUN      | Vec: tfidf_word_1-1\n",
      "{'acc': 0.7311891016550626, 'macro_f1': 0.7195677702933339}\n",
      "Prep: LOW+URLrem+PUN | Vec: tfidf_word_1-1\n",
      "{'acc': 0.7310299363859716, 'macro_f1': 0.7196131318136788}\n",
      "Prep: LOW+URLrep+PUN | Vec: tfidf_word_1-1\n",
      "{'acc': 0.7310299363859716, 'macro_f1': 0.7196131318136788}\n",
      "Prep: LOW+URLrem+PUN+RSW | Vec: tfidf_word_1-1\n",
      "{'acc': 0.7455158760599698, 'macro_f1': 0.7345545433858112}\n",
      "Prep: LOW+URLrep+PUN+RSW | Vec: tfidf_word_1-1\n",
      "{'acc': 0.7454628115866349, 'macro_f1': 0.7345090289779269}\n"
     ]
    }
   ],
   "source": [
    "full_data = fetch_20newsgroups(\n",
    "    subset=\"all\",\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    ")\n",
    "X, y = full_data.data, full_data.target\n",
    "print(\"Samples:\", len(X), \"Classes:\", set(y), \"Labels:\", full_data.target_names)\n",
    "\n",
    "VECTORIZERS = {\n",
    "    #\"count_word_1-1\": CountVectorizer(analyzer=\"word\", ngram_range=(1, 1), lowercase=False),\n",
    "    #\"count_word_1-2\": CountVectorizer(analyzer=\"word\", ngram_range=(1, 2), lowercase=False),\n",
    "    \"tfidf_word_1-1\": TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 1), lowercase=False),\n",
    "    #\"tfidf_word_1-2\": TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 2), lowercase=False),\n",
    "    #\"count_char_3-5\": CountVectorizer(analyzer=\"char\", ngram_range=(3, 5), lowercase=False),\n",
    "    #\"count_charwb_3-5\": CountVectorizer(analyzer=\"char_wb\", ngram_range=(3, 5), lowercase=False),\n",
    "    #\"tfidf_char_3-5\": TfidfVectorizer(analyzer=\"char\", ngram_range=(3, 5), lowercase=False),\n",
    "    #\"tfidf_charwb_3-5\": TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3, 5), lowercase=False),\n",
    "}\n",
    "\n",
    "rows_full = []\n",
    "for prep_name, prep in PREPROCESSORS.items():\n",
    "    for vec_name, vec in VECTORIZERS.items():\n",
    "        print(f\"Prep: {prep_name:12s} | Vec: {vec_name}\")\n",
    "        res = evaluate(X, y, prep, vec)\n",
    "        print(res)\n",
    "        rows_full.append({\n",
    "            \"preprocessing\": prep_name,\n",
    "            \"vectorizer\": vec_name,\n",
    "            **res\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50a3cb66-2ec4-45c7-96da-5926c19501c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         preprocessing      vectorizer       acc  macro_f1\n",
      "10  LOW+URLrem+PUN+RSW  tfidf_word_1-1  0.745516  0.734555\n",
      "11  LOW+URLrep+PUN+RSW  tfidf_word_1-1  0.745463  0.734509\n",
      "4                  RSW  tfidf_word_1-1  0.739148  0.728277\n",
      "5           LOW+URLrem  tfidf_word_1-1  0.731030  0.719613\n",
      "8       LOW+URLrem+PUN  tfidf_word_1-1  0.731030  0.719613\n",
      "9       LOW+URLrep+PUN  tfidf_word_1-1  0.731030  0.719613\n",
      "0                  LOW  tfidf_word_1-1  0.731189  0.719568\n",
      "7              LOW+PUN  tfidf_word_1-1  0.731189  0.719568\n",
      "6           LOW+URLrep  tfidf_word_1-1  0.730340  0.718766\n",
      "2               URLrep  tfidf_word_1-1  0.720895  0.710259\n",
      "1               URLrem  tfidf_word_1-1  0.720789  0.710122\n",
      "3                  PUN  tfidf_word_1-1  0.720364  0.709511\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(rows_full).sort_values(\"macro_f1\", ascending=False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec8ebb05-c365-450d-8574-ffb44f56d5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: results_20_classes.csv\n"
     ]
    }
   ],
   "source": [
    "OUTFILE2 = \"results_20_classes.csv\"\n",
    "df.to_csv(OUTFILE2, index=False)\n",
    "print(f\"Saved to: {OUTFILE2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e90c71-2f91-44a6-815f-84c9b66e4313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e15cc17-b24e-41db-97eb-031c481d58f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
